# Coupled Stochastic Variables

## Fun with Expectations

$$
\mathbb{E}[X|Y=y] = \sum_x P(X=x|Y=y)  = \int_\mathbb{R} xf_{x|y}(x|y) dx
$$

```{exercise}

A miner is lost in a cave. Facing north, he finds three caverns which seem to continue on. If he follows the trail through the first cavern, he will find his way back home in 3 hours. Through the second cavern, he will follow a circuitous route such that he will enter the cave facing north again. Through the third cavern, the same will occur but in 7 hours. The miner is very scared and cold and forgets which path he took every time he returns to the cave. He also never turns around to see the hole(s) that took him back to the cave. Find the expected time it takes for him to go to safety.

```

```{solution}

Imagine a Markov model with loops of 5 and 7 back to themselves with probability 1/3 each and an edge with 3 out to the final state.

$$
\mathbb{E}[X] = \sum_x xp(x)
$$

There's a 1/3 probability straight off the bat that he escapes the cave and doesn't have to deal with any of this nonsense. In this case, $xp(x)=3\cdot\frac{1}{3}=1$. However, there's a 2/3 chance that he takes 6 hours to come back (since the 5 and 7 hour paths have equal probabilities of being selected, given that the first path was not). From there, the process repeats. In essence, the expression looks like

$$
\frac{1}{3}(3) + \frac{2}{3}\left[ 6 + \frac{1}{3}(3) + \frac{2}{3}\left[ 6 + \frac{1}{3}(3) + \dots \right. \right.
$$

Given that the expression continues on infinitely, we cannot evaluate it as a closed-form expression. Let us instead consider generically the sum

$$
k(j+k(j + k(j + \dots
$$

If we write 

$$
f(n) = k \left[j + f(n) \right]
$$

Then

\begin{gather*}
f(n) = kj + k f(n) \\
f(n) - kf(n) = kj \\
f(n)(1-k) = kj \\
f(n) = \frac{kj}{1-k}
\end{gather*}

Using our values $k=2/3$ and $j=7$, we evaluate the entire expression from earlier as 15 hours.

```

```{exercise}

What is the expected number of cards one must flip to obtain 2 aces.

```

## Conditional Probability

Given the joint probability function $p(X=x,Y=y)$ and a function $Z=g(X,Y)$ for random variables $X$ and $Y$, can we find the probability function for $Z$?

```{definition, name = "Cumulative Distribution Function"}

$$
F_X(x) \equiv \Pr(X \leq x)
$$

```

Mathematically,
$$
F_X(x) = \int\limits_{-\infty}^x f_X(u) \dif u
$$

Therefore,

$$
f_X(x) = \od{F_X(x)}{x}
$$

So if we can find $F_X(x)$, we can get $f_X(x)$ pretty easily. Let $D$ be a region in the $XY$ plane where $g(x,y) \leq z$ is satisfied. Then,

$$
F_Z(z) = \int_{D} f(x,y) \dif A \implies f_Z(z) = \od{}{z} \int_{D} f(x,y) \dif A
$$

```{example}
Find the probability function for $Z=X+Y$.
```

```{solution}

\begin{align*}
f_Z(z) &= \od{}{z} \int_\RR\int_{-\infty}^{z-x} f(x,y) \dif y \dif x \\
&= \int_\RR \left[ \pd{}{z}\int_{-\infty}^{z-x} f(x,y) \dif y \right] \dif x \\
&= \int_\RR f(x, z - x) \dif x \\
\end{align*}

If we further assume $X$ and $Y$ are independent, then

$$
f_Z(z) = \int_\RR f_X(x)f_Y(z-x) \dif x = (f_Y * f_X)(z) = (f_X * f_Y)(z)
$$

```

## Relative Entropy & Mututal Information

```{definition, name = "Relative Entropy / Kullback-Leibler Divergence"}

$$
D_{KL}(P \parallel Q) \equiv \sum_x p(x) \log \frac{p(x)}{q(x)} = \mathbb{E}_p \left[ \log \frac{p(x)}{q(x)} \right]
$$

```

The KL divergence is the minimum possible inefficiency of using 

```{example}
Let $X$ be $\operatorname{Bern}(r)$ and $Y$ be $\operatorname{Bern}(s)$. Find $D_{KL}(X \parallel Y)$
```

```{solution}
$$
D(X \parallel Y) = (1-r)\log \frac{1-r}{1-s} + r\log \frac{r}{s}
$$
```

```{python, echo = FALSE}

from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
from matplotlib.ticker import LinearLocator, FormatStrFormatter



fig = plt.figure()
ax = fig.gca(projection='3d')

delta = 0.01
X = np.linspace(delta, 1 - delta)
Y = np.linspace(delta, 1 - delta)
X, Y = np.meshgrid(X, Y)
Z = (1-X) * np.log( (1-X)  / (1-Y) ) + X * np.log( X / Y)

# Plot the surface.
ax.plot_surface(X, Y, Z, linewidth=0, antialiased=False)

ax.view_init(elev=32, azim=-143)

plt.show()

```

```{definition, name = "Mutual Information"}
$$
I(X; Y) \equiv D_{KL}(p(x,y) \parallel p(x)p(y))
$$
```

```{lemma}
\begin{equation}
I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
(\#eq:mutual-info-cond)
\end{equation}
```

```{corollary}
$$
I(X;Y) = H(X) + H(Y) - H(X,Y)
$$
```


```{example}

Find $I(X; Y)$ for the table below. 

```


| |1|2|3|4|
|-|-|-|-|-|
|1|1/8|1/16|1/32|1/32|
|2|1/16|1/8|1/32|1/32|
|3|1/16|1/16|1/16|1/16|
|4|1/4|0|0|0|

Table: Joint Entropy for $X$ (columns) and $Y$ (rows)

```{solution}
$$
H(X) = \sum p(x) \log \frac{1}{p(x)} = 14/8
$$
  
$$
H(X|Y) = 11/8
$$
  
Then, $I(X;Y)=3/8$
```

```{theorem, name = "Chain Rule for Entropy"}

Let $X_1, \dots, X_n$ be drawn according to $p(x_1, \dots, x_n)$. Then,

$$
H(X_1, \dots, X_n) = \sum_{i=1}^n H(X_i | X_{i-1}, \dots, X_1)
$$

```

```{proof}
First, recall that

$$
H(X_1, X_2) = H(X_1) + H(X_2 | X_1)
$$
  
$$
H(X_1, X_2, X_3) = H(X_1) + H(X_2, X_3 | X_1)
$$
```

```{theorem, name = "Information Inequality"}

$$
D(p \parallel q) \geq 0
$$

```

```{proof}
\begin{align*}
-D_{KL}(p \parallel q) &= - \sum p(x) \log \frac{p(x)}{q(x)} \\
&= \sum p(x) \log \frac{q(x)}{p(x)} \\
&\leq \log \sum p(x) \frac{q(x)}{p(x)} \\
&= \log \sum q(x) \\
&= \log 1 \\
&= 0
\end{align*}

$$
-D_{KL}(p \parallel q) \leq 0 \implies D_{KL}(p \parallel q) \geq 0
$$
```

```{corollary}
$$
  I(X; Y) \geq 0
$$
```

```{exercise}

Show that $H(X) \leq \log \lvert \mathscr{H}\rvert$. $\lvert \mathscr{H} \rvert$ denotes the number of elements in the range of $X$ of equality iff X has a uniform distribution.

```

```{proof}

\begin{align*}
D_{KL}(p \parallel \mathscr{U}) &= \mathbb{E}_p \left[\log\frac{p(x)}{q(x)}\right] \\
&= \mathbb{E}_p \left[ \log \left(\lvert\mathscr{H}\rvert p(x)\right)  \right] \\
&= \sum p(x) \log \left(\lvert\mathscr{H}\rvert p(x)\right) \\
&= \sum p(x) \left( \log \lvert \mathscr{H} \rvert + \log p(x)  \right) \\
&= \sum p(x) \log \lvert \mathscr{H} \rvert + \sum p(x) \log p(x) \\
&= \log \lvert \mathscr{H} \rvert - H(X)
\end{align*}

```


```{lemma}

The following statement holds with equality iff $X$ and $Y$ are independent. 

$$
H(X|Y) \leq H(X)
$$
  
```

```{marginfigure}
_Intuition_. Knowing another R.V. value cannot, on average, increase the surprisal of a R.V.
```

```{exercise}
Do something with the table below.
```

|X|1|2|
|-|-|-|
|Y| | |
|1| 0 | 3/4|
|2| 1/8 | 1/8|

```{solution}

$$
H(X) = \sum p(x) \log \frac{1}{p(x)} = \frac{1}{8} \log 8 + \frac{7}{8}\log \frac{8}{7} \approx .544
$$

$$
H(X | Y = 1) = \frac{3}{4}\log \frac{4}{3} \approx .311
$$
  
$$
H(X | Y = 2) = 1
$$
  
$$
H(X|Y) \approx 0.483
$$

```